# -*- coding: utf-8 -*-
"""messy data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sOr1d7MX4W0CqfqT7vqYLXaWB_Omi187
"""

!pip install pyspark

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, split, regexp_replace

spark = SparkSession.builder.appName("DataCleaning").getOrCreate()

df = spark.read.option("delimiter", "\t").option("header", True).csv("population_by_age.tsv.txt")
df.show(5)

df = df.withColumnRenamed("indic_de,geo\\time", "indicator_country")
df.show(5)

# Split using the comma in 'indicator_country'
df = df.withColumn("indicator", split(col("indicator_country"), ",").getItem(0))
df = df.withColumn("country", split(col("indicator_country"), ",").getItem(1))

# Drop the old messy column
df = df.drop("indicator_country")
df.show(5)

# Trim spaces from column names
for colname in df.columns:
    df = df.withColumnRenamed(colname, colname.strip())

# List of all year columns to clean
year_columns = ['2008', '2009', '2010', '2011', '2012', '2013', '2014',
                '2015', '2016', '2017', '2018', '2019']

# Use regexp_replace to remove anything that is NOT a digit or dot
for year in year_columns:
    df = df.withColumn(year, regexp_replace(col(year), "[^0-9.]", ""))

df.select("indicator", "country", "2008", "2009", "2010").show(10)

for year in year_columns:
    df = df.withColumn(year, col(year).cast("float"))

from pyspark.sql.functions import isnan, when, count

df.select([
    count(when(col(c).isNull(), c)).alias(c) for c in year_columns
]).show()

df = df.fillna(0)

df.show(10)

df.write.mode("overwrite").option("header", True).csv("cleaned_population")

import shutil
shutil.make_archive("cleaned_population", 'zip', "cleaned_population")

from google.colab import files
files.download("cleaned_population.zip")